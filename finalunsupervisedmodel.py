# -*- coding: utf-8 -*-
"""FinalUnsupervisedModel.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17ANwY3LbjJLWJ7Bn5P9Oy7NZkpxSys2r

# The data and project overview

NYC data set consists of 4 separate data sets with data for all public transit bus lines in the NYC area.

The dataset consists of nearly 4 million rows of data. After data reprocessing and feature engineering, the data consists of 366 features. 

The Kmeans and DBSCAN clustering methods were applied to the data, along with applying Tsne and PCA. For reference, I also applied different levels of scaling, and applied clustering to a dataset to a reduced feature selection, not incorporating the Dummies library.

Build environment then load data
"""

import pandas as pd
import numpy as np
import time
from datetime import datetime
import datetime as dt
import warnings
warnings.filterwarnings('ignore')
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt

from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.cluster import KMeans
from sklearn.cluster import DBSCAN
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import seaborn as sns
from sklearn import datasets, metrics
from sklearn.preprocessing import StandardScaler, MinMaxScaler, normalize

# Load dataset that has been cleaned and reprocessed in Data cleaning notebook
Dates = pd.read_csv("Dates.csv", dtype='int8')

# Verifying Dataset loaded correctly with similar shape and file size
Dates.info()

# Due to time constraints, only 10% of total data is used
# I also should have reduced the file size of the numbers but did not.
Dates = Dates.sample(frac=.1)
Dates.reset_index(drop=True, inplace=True)

# I filter out only buses that are late so
# we can determine what jeay factors determine this result.
# The dataset size is small but the features are currently intact.
Dates = Dates.loc[Dates['Late_yes'] == 1]
Dates.drop(['Late_yes'],axis=1, inplace=True)
Dates.reset_index(drop=True, inplace=True)
Dates.info()

# I choose top drop the Unamed field, the Early field since this
# is not a question we planned to address, and the Distance from stop
# due to identifying an issue where buses that have already
# passed the destination were also being included which resulted
# in a negative deistance value
Dates.drop(['Unnamed: 0', 'Early_yes'], axis=1, inplace=True)
Dates.drop(['DistanceFromStop'], axis=1, inplace=True)
Dates.reset_index(drop=True, inplace=True)
Dates.info()

# Just confirming the data is as expected
Dates.head()

# In this instance I am using the variable X to include all data,
# y will be referenced during visualization of clusters.
# I also elected to use Standard Scaling. I use different scaling methods
# in other models we will address later
X = Dates

ms = StandardScaler()
X = ms.fit_transform(X)

"""# Test K values for Kmeans"""

# In this test I tried many time to find an optimal K for Kmeans
# However the testing ultimately was not very good.
sse = {}
for k in range(2, 10):
    kmeans = KMeans(n_clusters=k, max_iter=300).fit(X)
    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.ylabel("SSE")
plt.show()

"""# Plot silhouette scores visually"""

# the silhoeutte scores show an inability
# to distinguish the data in clusters from one to another
silhouette_scores = [] 

for n_cluster in range(2, 9):
    silhouette_scores.append( 
        silhouette_score(X, KMeans(n_clusters = n_cluster).fit_predict(X))) 
    
# Plotting a bar graph to compare the results 
k = [2,3,4,5,6,7,8] 
plt.bar(k, silhouette_scores) 
plt.xlabel('Number of clusters', fontsize = 10) 
plt.ylabel('Silhouette Score', fontsize = 10) 
plt.show()

"""# Closer look at silhouette score values"""

# once again confirming the scores are near zero
range_n_clusters = [2,3,4,5]

for n_clusters in range_n_clusters:
    cluster = KMeans(n_clusters=n_clusters, random_state=1)
    cluster_labels = cluster.fit_predict(X)
    
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters = ", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
    
    sample_silhoeutte_values = silhouette_samples(X, cluster_labels)

"""# PCA dimentionality reduction"""

# I am applying PCA to visualize the clustering
pca = PCA(n_components = 2) 
X_pca = pca.fit_transform(X) 
X_pca = pd.DataFrame(X_pca) 
X_pca.columns = ['P1', 'P2'] 
  
X_pca.head(2)

"""# Tsne dimentionality reduction"""

# I am applying Tsne to also visualize the clustering
tsne = TSNE(n_components = 2, verbose=1, perplexity=40, n_iter=300) 
X_tsne = tsne.fit_transform(X) 
X_tsne = pd.DataFrame(X_tsne) 
X_tsne.columns = ['P1', 'P2'] 
  
X_tsne.head(2)

"""# Visualizing the clusters"""

# Here we apply a cluster of 2 to K and see no noticeable difference
# between the clusters
plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 2).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 2(PCA applied)')
plt.show()

"""# Visualize all K clusters 2 through 4 with PCA"""

# Clustering of 3 provides no value at all
plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 3).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 3(PCA applied)')
plt.show()

# Same result as before, but we can at least see 2 of the clusters
plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 4).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 4(PCA applied)')
plt.show()

"""# Visualize all K clusters 2 through 4 withTsne"""

# Visuallizing the clusters using Tsne provided no value
plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 2).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 2(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 3).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 3(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 4).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 4(Tsne applied)')
plt.show()

# ultimately the interesting story here is the clusters
# are all surrounding and on top of each other, but
# nothing discernable can be taken away from this at this time
plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 5).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

"""With full features, and standard scaling, the clustering techniques could not provide value in this instance and are not recommended.

#Data reprocessing to remove number of features and apply Label Encoding instead
"""

Date1 = pd.read_csv("fullmta_1706.csv")
Date2 = pd.read_csv("fullmta_1708.csv")
Date3 = pd.read_csv("fullmta_1710.csv")
Date4 = pd.read_csv("fullmta_1712.csv")

Dates = pd.concat([Date1,Date2,Date3,Date4])

del Date1
del Date2
del Date3
del Date4

Dates.drop(['None','OriginLat','OriginLong','OriginName','DestinationLat','DestinationLong','VehicleLocation.Latitude','VehicleLocation.Longitude','ArrivalProximityText'], axis=1, inplace=True)

Dates.apply(pd.to_numeric, errors='coerce')
Dates.fillna(0, inplace=True)

Dates['DistanceFromStop'].replace('at stop|approaching|< 1 stop away','0',regex=True, inplace=True)
Dates['DistanceFromStop'] = Dates['DistanceFromStop'].astype(int)

test = Dates['DestinationName'].value_counts()
Dates['DestinationName'] = np.where(Dates['DestinationName'].isin(test.index[test < 2000]), 'Other', Dates['DestinationName'])

def convertRecTime(x):
  Dates['RecTimeParsed'] = pd.to_datetime(x, format='%m/%d/%Y %H:%M')
  Dates['RecTime'] = Dates['RecTimeParsed'].dt.time
  return 

def convertSchedTime(x):
  Dates['SchedTimeParsed'] = pd.to_datetime(Dates['ScheduledArrivalTime'], format='%H:%M:%S', errors='coerce')
  Dates['SchedTime'] = Dates['SchedTimeParsed'].dt.time
  return 

def convertExpecTime(x):
  Dates['ExpecTimeParsed'] = pd.to_datetime(Dates['ExpectedArrivalTime'], format='%m/%d/%Y %H:%M', errors='coerce')
  Dates['ExpecTime'] = Dates['ExpecTimeParsed'].dt.time
  return

convertRecTime(Dates['RecordedAtTime'])
convertSchedTime(Dates['ScheduledArrivalTime'])
convertExpecTime(Dates['ExpectedArrivalTime'])

Dates.drop(['ExpectedArrivalTime','ScheduledArrivalTime','RecordedAtTime','RecTime','SchedTime','ExpecTime','NextStopPointName','VehicleRef'],axis=1, inplace=True)
Dates['Day'] = Dates['RecTimeParsed'].dt.day_name()
Dates['Hour'] = Dates['RecTimeParsed'].dt.hour

Dates.drop(['RecTimeParsed'],axis=1, inplace=True)

indexName = Dates[Dates['Hour'] > 22].index
Dates.drop(indexName, inplace=True)
indexName2 = Dates[Dates['Hour'] < 3].index
Dates.drop(indexName2, inplace=True)

Dates['SecondsLate'] = (Dates['ExpecTimeParsed'].dt.second + Dates['ExpecTimeParsed'].dt.minute * 60 + Dates['ExpecTimeParsed'].dt.hour * 3600) - (Dates['SchedTimeParsed'].dt.second + Dates['SchedTimeParsed'].dt.minute * 60 + Dates['SchedTimeParsed'].dt.hour * 3600)

Dates['Late_yes'] = [1 if seconds > 900 else 0 for seconds in Dates['SecondsLate']]
Dates['Early_yes'] = [1 if seconds < 0 else 0 for seconds in Dates['SecondsLate']]

Dates.drop(['SchedTimeParsed','ExpecTimeParsed','SecondsLate', 'Early_yes'],axis=1, inplace=True)

Dates['PublishedLineName'] = Dates['PublishedLineName'].str.split('9|8|7|6|5|4|3|2|1|0').str[0]

Dates.reset_index(drop=True, inplace=True)

Dates = Dates.sample(frac=.1)
Dates.reset_index(drop=True, inplace=True)
Dates['DirectionRef'] = Dates['DirectionRef'].astype('int8')
Dates['DistanceFromStop'] = Dates['DistanceFromStop'].astype('int8')
Dates['Hour'] = Dates['Hour'].astype('int8')
Dates['Late_yes'] = Dates['Late_yes'].astype('int8')
Dates.info()

# I define X now and define y later. 
# I also apply label encoding here since the three columns were
# not addressed in one hot encoding this time around

X = Dates

le = LabelEncoder()

X['PublishedLineName'] = le.fit_transform(X['PublishedLineName'])
X['Day'] = le.fit_transform(X['Day'])
X['DestinationName'] = le.fit_transform(X['DestinationName'])

ms = StandardScaler()
X = ms.fit_transform(X)
X = normalize(X)

"""# Test K values for Kmeans"""

# Visually attempting to determine K
sse = {}
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X)
    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.ylabel("SSE")
plt.show()

"""# Plot silhouette scores visually"""

# The clusters are slightly improved this time
# around however, still not ideal
silhouette_scores = [] 

for n_cluster in range(2, 8):
    silhouette_scores.append( 
        silhouette_score(X, KMeans(n_clusters = n_cluster).fit_predict(X))) 
    
# Plotting a bar graph to compare the results 
k = [2, 3, 4, 5, 6,7] 
plt.bar(k, silhouette_scores) 
plt.xlabel('Number of clusters', fontsize = 10) 
plt.ylabel('Silhouette Score', fontsize = 10) 
plt.show()

"""# Closer look at silhouette score values"""

# The highest silhouette score pertains to clusters 5-8, unfortunately.
range_n_clusters = [2,3,4,5,6,7,8]

for n_clusters in range_n_clusters:
    cluster = KMeans(n_clusters=n_clusters, random_state=1)
    cluster_labels = cluster.fit_predict(X)
    
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters = ", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
    
    sample_silhoeutte_values = silhouette_samples(X, cluster_labels)

"""# PCA dimentionality reduction"""

# Applying PCA for visualization purposes
pca = PCA(n_components = 2) 
X_pca = pca.fit_transform(X) 
X_pca = pd.DataFrame(X_pca) 
X_pca.columns = ['P1', 'P2'] 
  
X_pca.head(2)

"""# Tsne dimentionality reduction"""

# Applying Tsne for visualization purposes
tsne = TSNE(n_components = 2, verbose=1, perplexity=40, n_iter=300) 
X_tsne = tsne.fit_transform(X) 
X_tsne = pd.DataFrame(X_tsne) 
X_tsne.columns = ['P1', 'P2'] 
  
X_tsne.head(2)

# It looks as though we can finally see two clear clusters,
# however I will try to visualize all eight due to the
# silhouette scoring outcome
plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 2).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 2(PCA applied)')
plt.show()

"""# Visualize all K clusters 2 through 8 with PCA"""

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 3).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 3(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 4).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 4(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 5).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 5(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 6).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 6(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 7).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 7(PCA applied)')
plt.show()

# Visually, we really cannot tell the difference between the two clusters.
plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 8).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 8(PCA applied)')
plt.show()

"""# Visualize all K clusters 2 through 8 withTsne"""

# We simply cannot tell the difference between clusters using this reduction technique
plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 2).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 2(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 3).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 3(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 4).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 4(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 5).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 6).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 6(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 7).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 7(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 8).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 8(Tsne applied)')
plt.show()

"""# Using the same data, I apply MinMaxScaler as opposed to Standard Scaler."""

# this time I apply MinMaxScaler to our features to see if ths makes any imporvement
X = Dates

le = LabelEncoder()

X['PublishedLineName'] = le.fit_transform(X['PublishedLineName'])
X['Day'] = le.fit_transform(X['Day'])
X['DestinationName'] = le.fit_transform(X['DestinationName'])

#cols = X.columns

ms = MinMaxScaler()
X = ms.fit_transform(X)
X = normalize(X)
#X = pd.DataFrame(X, columns=[cols])

"""# Test K values for Kmeans"""

# Theres is some improvement! We can see a hard bend,
# at a value of 2 however for K
sse = {}
for k in range(1, 10):
    kmeans = KMeans(n_clusters=k, max_iter=100).fit(X)
    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center
plt.figure()
plt.plot(list(sse.keys()), list(sse.values()))
plt.xlabel("Number of cluster")
plt.ylabel("SSE")
plt.show()

"""# Plot silhouette scores visually"""

silhouette_scores = [] 

for n_cluster in range(2, 8):
    silhouette_scores.append( 
        silhouette_score(X, KMeans(n_clusters = n_cluster).fit_predict(X))) 
    
# Plotting a bar graph to compare the results 
k = [2, 3, 4, 5, 6,7] 
plt.bar(k, silhouette_scores) 
plt.xlabel('Number of clusters', fontsize = 10) 
plt.ylabel('Silhouette Score', fontsize = 10) 
plt.show()

"""# Closer look at silhouette score values"""

# The value of 2 for K does show a better silhouette score
# comapred to all other testing so far
range_n_clusters = [2,3,4,5,6,7,8]

for n_clusters in range_n_clusters:
    cluster = KMeans(n_clusters=n_clusters, random_state=1)
    cluster_labels = cluster.fit_predict(X)
    
    silhouette_avg = silhouette_score(X, cluster_labels)
    print("For n_clusters = ", n_clusters,
          "The average silhouette_score is :", silhouette_avg)
    
    sample_silhoeutte_values = silhouette_samples(X, cluster_labels)

"""# Visualize all K clusters 2 through 8 with PCA"""

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 2).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 2(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 3).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 3(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 4).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 4(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 5).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 5(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 6).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 6(PCA applied)')
plt.show()

plt.scatter(X_pca['P1'], X_pca['P2'],  
           c = KMeans(n_clusters = 7).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 7(PCA applied)')
plt.show()

"""# Visualize all K clusters 2 through 8 withTsne"""

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 2).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 2(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 3).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 3(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 4).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 4(Tsne applied)')
plt.show()

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = KMeans(n_clusters = 5).fit_predict(X), cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

"""# Color Target variable to identify plots"""

# Now that I have the best performing clustering method for
# Kmeans, I try to identify the larger target variable
# in each cluster
# In this instace the Published Line name is not veyr helpful
y = Dates['PublishedLineName']

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = y, cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

# No story to follow here with the day of the week either
y = Dates['Day']

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = y, cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

# with this being a new feature, this represents each hour of
# of the day, in hopes of identifying the largest culprit.
# Although the concept is sound, application in my model needs improvement.
y = Dates['Hour']

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = y, cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

# highlighting for Destination did not help much
y = Dates['DestinationName']

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = y, cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

# Here we see the clusters were seprated in fact by
# the direction the buses were headed, this being
# defined as inbound or outbound.
y = Dates['DirectionRef']

plt.scatter(X_tsne['P1'], X_tsne['P2'],  
           c = y, cmap =plt.cm.winter) 
plt.title('K-means value of 5(Tsne applied)')
plt.show()

"""#DBSCAN clustering"""

# Due to time constraints I am unable to test DBSCAN with full feature selection
# so only the dataset that was created in this notebook will be used.
# I also will reapply the previosuly established PCA and 
# Tsne reduced datasets for visualization purposes
X = Dates

le = LabelEncoder()

X['PublishedLineName'] = le.fit_transform(X['PublishedLineName'])
X['Day'] = le.fit_transform(X['Day'])
X['DestinationName'] = le.fit_transform(X['DestinationName'])

ms = StandardScaler()
X = ms.fit_transform(X)

"""# Hyperparameter tuning for DBSCAN"""

# Testing a range from 0.5 thru 2. 
# Anything greater than 2 resulted in an error.
# Looking ahead the scores are not as good as Kmeans,
# but also not as bad as the first methods.
range_eps = [0.5,1,1.5,2]

for i in range_eps:
    print("eps value is "+str(i))
    db = DBSCAN(eps=i, min_samples=10).fit(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_
    print(set(labels))  
    silhouette_avg = silhouette_score(X, labels)
    print("For eps value ="+str(i), labels,
          "The average silhouette_score is :", silhouette_avg)

# No difference in the min_sample selection so 
# I will use only a value of 1
min_samples = [1,2,3,4,5]
for i in min_samples :
    print("Min_samples values is "+str(i))
    db = DBSCAN(eps=1.5, min_samples=i).fit(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True   
    
    labels = set([label for label in db.labels_ if label >= 0])
    print(set(labels))
    print("For min_samples value ="+str(i), "Total no. of clusters are "+ str(len(set(labels))))

"""# Visualize Data with PCA"""

# Visuallizing the data shows we can determine which are
# late and which are not which is good. If coming to this data
# without knowing, we can spot the difference, but this was not the 
# question we planned to address, we want to identify clusters
# within the late instances.
y = Dates['Late_yes']

dbscan_cluster = DBSCAN(eps=1.5, min_samples=1, metric='euclidean')

clusters = dbscan_cluster.fit_predict(X)

plt.figure(figsize=(10,5))
colours = 'rbg'
for i in range(pca.shape[0]):
    plt.text(pca[i,0], pca[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

# The bus lines are thoroughly scattered within these clusters
y = Dates['PublishedLineName']

plt.figure(figsize=(10,5))
colours = 'rbgyrmbkcgw'
for i in range(pca.shape[0]):
    plt.text(pca[i,0], pca[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

# Attempting to identify the day provides no value
y = Dates['Day']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(pca.shape[0]):
    plt.text(pca[i,0], pca[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

"""# Visualize data with Tsne"""

# in short, applying the Tsne reduction for the same
# visualization methods provides no value
y = Dates['Late_yes']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(X_tsne.shape[0]):
    plt.text(X_tsne[i,0], X_tsne[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

y = Dates['PublishedLineName']

plt.figure(figsize=(10,5))
colours = 'rbgyrmbkcgw'
for i in range(X_tsne.shape[0]):
    plt.text(X_tsne[i,0], X_tsne[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

y = Dates['Day']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(X_tsne.shape[0]):
    plt.text(X_tsne[i,0], X_tsne[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.show()

"""# DBSCAN clustering using Using MinMaxScaler as opposed to Standard Scaler."""

X = Dates

le = LabelEncoder()

X['PublishedLineName'] = le.fit_transform(X['PublishedLineName'])
X['Day'] = le.fit_transform(X['Day'])
X['DestinationName'] = le.fit_transform(X['DestinationName'])

ms = MinMaxScaler()
X = ms.fit_transform(X)

"""# Hyperparameter tuning for DBSCAN"""

# In this hyperparamter testing any score of 1 or higher resulted in an error
# the epsilpn score of 0.5 performed best with regards to
# the silhouette score.
range_eps = [0.1,0.3,0.5,0.8]

for i in range_eps:
    print("eps value is "+str(i))
    db = DBSCAN(eps=i, min_samples=10).fit(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True
    labels = db.labels_
    print(set(labels))  
    silhouette_avg = silhouette_score(X, labels)
    print("For eps value ="+str(i), labels,
          "The average silhouette_score is :", silhouette_avg)

# With min_samples showing again no change from 1-5, I elect to use 1
min_samples = [1,2,3,4,5]
for i in min_samples :
    print("Min_samples values is "+str(i))
    db = DBSCAN(eps=0.5, min_samples=i).fit(X)
    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)
    core_samples_mask[db.core_sample_indices_] = True   
    
    labels = set([label for label in db.labels_ if label >= 0])
    print(set(labels))
    print("For min_samples value ="+str(i), "Total no. of clusters are "+ str(len(set(labels))))

"""# Visualize Data with PCA"""

# Visuallizing the Published Line provides no value
y = Dates['PublishedLineName']

dbscan_cluster = DBSCAN(eps=.5, min_samples=1, metric='euclidean')

clusters = dbscan_cluster.fit_predict(X)

plt.figure(figsize=(10,5))
colours = 'rbgyrmbkcgw'
for i in range(pca.shape[0]):
    plt.text(pca[i,0], pca[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.title('DBSCAN with PublishedLineName highlighted(PCA applied)')
plt.show()

# We are able to identify each day but this ultimately shows no
# major cluster or true impact of one or several days as leading
# cause of overall lateness
y = Dates['Day']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(pca.shape[0]):
    plt.text(pca[i,0], pca[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.title('DBSCAN with Day highlighted(PCA applied)')
plt.show()

# No value here with even distribution for inbound and
# outbound bus delays
y = Dates['DirectionRef']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(pca.shape[0]):
    plt.text(pca[i,0], pca[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.title('DBSCAN with Day highlighted(PCA applied)')
plt.show()

"""# Visualize data with Tsne"""

# No value can be ascertained from this last decution technique.
y = Dates['PublishedLineName']

plt.figure(figsize=(10,5))
colours = 'rbgyrmbkcgw'
for i in range(X_tsne.shape[0]):
    plt.text(X_tsne[i,0], X_tsne[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.title('DBSCAN with PublishedLineName highlighted(Tsne applied)')
plt.show()

y = Dates['Day']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(X_tsne.shape[0]):
    plt.text(X_tsne[i,0], X_tsne[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.title('DBSCAN with Day highlighted(Tsne applied)')
plt.show()

y = Dates['DirectionRef']

plt.figure(figsize=(10,5))
colours = 'rbgyrmb'
for i in range(X_tsne.shape[0]):
    plt.text(X_tsne[i,0], X_tsne[i,1], str(clusters[i]),
            color=colours[y[i]],
            fontdict={'weight': 'bold', 'size': 50}
            )
    
plt.xticks([])
plt.yticks([])
plt.axis('off')
plt.title('DBSCAN with Day highlighted(Tsne applied)')
plt.show()

"""#Results Analysis

Both the Kmeans and DBSCAN approach to clustering resulted in 2 clear clusters over all, however in this instance we could identify only the direction the buses were headed. The best silhoette score came from the DBSCANclustering model with MinMaxScaler applied at a score of 0.36. It was easy to idnetify the direction of course, and also the days of the week, but beyond that there was nothing of value.

#The final product

Ultimately none of these models are ready for production, and the key issue is likely in feature engineering. This issue was addressed slightly in the Supervised model final product review, but in this case where identifying major "hot spots" in the data is of value, these clustering methods require more time and work.
"""