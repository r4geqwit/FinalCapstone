{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"NYCbusdatacleanandexport.ipynb","provenance":[],"collapsed_sections":["jebLZHqIJpiM","ChLyxe_CdXQo","mnBUVHJTfB40","7zGUV65Cl5h2","7IFcgpQbe6rh"],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"UgzsgBJ0oUS1"},"source":["Source of data: https://www.kaggle.com/stoney71/new-york-city-transport-statistics\n","\n","Four CSV files totalling 5GB in size. \n","\n","Due to size of data, files will be loaded on Google Drive and not Github.\n","\n","Each data set contained a last column with no header. Due to this, a header name was provided \"None\" to bypass errors loading data into a dataframe."]},{"cell_type":"markdown","metadata":{"id":"CGiIPT5CyPxD"},"source":["Facts about data\n","\n","This data shows the first 5 business days of the months June 2017, August 2017, October 2017, and December 2017, with 10 minute increments, of all NYC public transit.\n","\n","I need to split DF for Buses and trains."]},{"cell_type":"markdown","metadata":{"id":"jebLZHqIJpiM"},"source":["#**Build Environment and EDA**"]},{"cell_type":"code","metadata":{"id":"BuMUtodogUIE"},"source":["import pandas as pd\n","import numpy as np\n","import time\n","from datetime import datetime\n","import datetime as dt\n","\n","import matplotlib.pyplot as plt\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report, confusion_matrix, recall_score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.svm import LinearSVC\n","from sklearn.model_selection import cross_val_score\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn import tree, ensemble\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import classification_report \n","\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"B1dR4kMjkjZg"},"source":["#**CSV1**"]},{"cell_type":"code","metadata":{"id":"ebaYPDfG7_7V"},"source":["# Load each CSV from local repository to Notebook\n","# then merge all into one dataframe\n","Date1 = pd.read_csv(\"fullmta_1706.csv\")\n","Date2 = pd.read_csv(\"fullmta_1708.csv\")\n","Date3 = pd.read_csv(\"fullmta_1710.csv\")\n","Date4 = pd.read_csv(\"fullmta_1712.csv\")\n","\n","Datesfull = pd.concat([Date1,Date2,Date3,Date4])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2IctexPt_ZnD","executionInfo":{"elapsed":588,"status":"ok","timestamp":1604797524558,"user":{"displayName":"Taco Cat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwDT7Fd6ACtx22TxBnnnogMSGkapVQVe2K8C2q=s64","userId":"04847116057603355161"},"user_tz":360},"outputId":"48f03c11-40dd-4e65-99fb-df289cdfbb87"},"source":["# Reset index on merged file, \n","# we here then see over 4 million rows of data\n","Datesfull.reset_index(drop=True, inplace=True)\n","Datesfull.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 4194300 entries, 0 to 4194299\n","Data columns (total 18 columns):\n"," #   Column                     Dtype  \n","---  ------                     -----  \n"," 0   RecordedAtTime             object \n"," 1   DirectionRef               float64\n"," 2   PublishedLineName          object \n"," 3   OriginName                 object \n"," 4   OriginLat                  float64\n"," 5   OriginLong                 float64\n"," 6   DestinationName            object \n"," 7   DestinationLat             float64\n"," 8   DestinationLong            float64\n"," 9   VehicleRef                 object \n"," 10  VehicleLocation.Latitude   float64\n"," 11  VehicleLocation.Longitude  float64\n"," 12  NextStopPointName          object \n"," 13  ArrivalProximityText       object \n"," 14  DistanceFromStop           object \n"," 15  ExpectedArrivalTime        object \n"," 16  ScheduledArrivalTime       object \n"," 17  None                       object \n","dtypes: float64(7), object(11)\n","memory usage: 576.0+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"X3jPHQkH_ZnH"},"source":["# Due to local system memory constraints, file is reduced by 50%\n","Dates = Datesfull.sample(frac=.50)\n","Dates.reset_index(drop=True, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Hdznx5pWB4v"},"source":["# deleting previous dataframe to clear memory space\n","del Datesfull"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UDmDH93HRNmG","executionInfo":{"elapsed":6625,"status":"ok","timestamp":1604797530623,"user":{"displayName":"Taco Cat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwDT7Fd6ACtx22TxBnnnogMSGkapVQVe2K8C2q=s64","userId":"04847116057603355161"},"user_tz":360},"outputId":"9b584aee-b934-4f09-8759-9c321ac138f8"},"source":["# New dataframe consists of 2 million rows and 18 features\n","Dates.info()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 2097150 entries, 0 to 2097149\n","Data columns (total 18 columns):\n"," #   Column                     Dtype  \n","---  ------                     -----  \n"," 0   RecordedAtTime             object \n"," 1   DirectionRef               float64\n"," 2   PublishedLineName          object \n"," 3   OriginName                 object \n"," 4   OriginLat                  float64\n"," 5   OriginLong                 float64\n"," 6   DestinationName            object \n"," 7   DestinationLat             float64\n"," 8   DestinationLong            float64\n"," 9   VehicleRef                 object \n"," 10  VehicleLocation.Latitude   float64\n"," 11  VehicleLocation.Longitude  float64\n"," 12  NextStopPointName          object \n"," 13  ArrivalProximityText       object \n"," 14  DistanceFromStop           object \n"," 15  ExpectedArrivalTime        object \n"," 16  ScheduledArrivalTime       object \n"," 17  None                       object \n","dtypes: float64(7), object(11)\n","memory usage: 288.0+ MB\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"LKmHJbAZm7ep"},"source":["# Dropping coordinate fields due to them not being used\n","# Dropping proximaty alert text due to redundancy in other features\n","# Dropping None field as it provides no explanation of purpose\n","Dates.drop(['None','OriginLat','OriginLong','OriginName','DestinationLat','DestinationLong','VehicleLocation.Latitude','VehicleLocation.Longitude','ArrivalProximityText'], axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVaCP6e921z3"},"source":["# Convert all destination names into potentially useful features\n","# with the help of one-hot encoding\n","test = Dates['DestinationName'].value_counts()\n","Dates['DestinationName'] = np.where(Dates['DestinationName'].isin(test.index[test < 2000]), 'Other', Dates['DestinationName'])\n","dummy = pd.get_dummies(Dates['DestinationName'], drop_first=True)\n","Dates = pd.concat([Dates, dummy], axis=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jNURGcGyAxkP"},"source":["# Converting the Published Line name to a new feature that\n","# removes the number, giving us only the route acronym, then\n","# applying one-hot encoding. Lastly dropping the original feature column\n","Dates = Dates.join(pd.get_dummies(Dates['PublishedLineName'].str.split('9|8|7|6|5|4|3|2|1|0').str[0],drop_first=True))\n","\n","Dates.drop(['PublishedLineName'],axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"voHns8Z2vL_N"},"source":["# During previous reprocessing, NaN values were somehow created, those are being dropped.\n","# Replacing Any value in the column Distance from stop that is considered\n","# \"very close to arrival\" to an interger of zero. Also converting to a smaller integer format.\n","Dates.dropna(inplace=True)\n","Dates['DistanceFromStop'].replace('at stop|approaching|< 1 stop away','0 ',regex=True, inplace=True)\n","Dates['DistanceFromStop'] = Dates['DistanceFromStop'].astype(int)\n","\n","#Dates = Dates.dropna(axis=0, subset=['DirectionRef'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Wf5cOjR_ZnR"},"source":["# Defining functions whose purpose is to reprocess the date time stamps\n","# located under Recorded Time, Scheduled Time, and Expected Time.\n","# Lastly dropping the original column names. \n","def convertRecTime(x):\n","  Dates['RecTimeParsed'] = pd.to_datetime(x, format='%m/%d/%Y %H:%M')\n","  Dates['RecTime'] = Dates['RecTimeParsed'].dt.time\n","  return \n","\n","def convertSchedTime(x):\n","  Dates['SchedTimeParsed'] = pd.to_datetime(Dates['ScheduledArrivalTime'], format='%H:%M:%S', errors='coerce')\n","  Dates['SchedTime'] = Dates['SchedTimeParsed'].dt.time\n","  return \n","\n","def convertExpecTime(x):\n","  Dates['ExpecTimeParsed'] = pd.to_datetime(Dates['ExpectedArrivalTime'], format='%m/%d/%Y %H:%M', errors='coerce')\n","  Dates['ExpecTime'] = Dates['ExpecTimeParsed'].dt.time\n","  return\n","\n","convertRecTime(Dates['RecordedAtTime'])\n","convertSchedTime(Dates['ScheduledArrivalTime'])\n","convertExpecTime(Dates['ExpectedArrivalTime'])\n","\n","Dates.drop(['ExpectedArrivalTime','ScheduledArrivalTime','RecordedAtTime','RecTime','SchedTime','ExpecTime','DestinationName','NextStopPointName','VehicleRef'],axis=1, inplace=True)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"owypxsKG_ZnU"},"source":["# Converting the newly created time for recorded date into\n","# the specific day and hour for potential use later. Also\n","# applying one-hot encoding to newly created features.\n","\n","Dates['Day'] = Dates['RecTimeParsed'].dt.day_name()\n","Dates['Hour'] = Dates['RecTimeParsed'].dt.hour\n","Dates = Dates.join(pd.get_dummies(Dates['Day'],drop_first=True))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNgFjhT-_ZnW"},"source":["# Dropping feature labeled recorded time\n","Dates.drop(['RecTimeParsed'],axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QsmL__bs_ZnX"},"source":["# During previous analysis, serious issue was detected\n","# with overlaps of time crossing over from one day\n","# to the other. due to this, I am removing all arrival times\n","# within 5 hours of midnight.\n","# Lastly, defining \"late\" and \"Early\" which will be used to identify\n","# later in model buidling\n","indexName = Dates[Dates['Hour'] > 22].index\n","Dates.drop(indexName, inplace=True)\n","indexName2 = Dates[Dates['Hour'] < 3].index\n","Dates.drop(indexName2, inplace=True)\n","\n","Dates['SecondsLate'] = (Dates['ExpecTimeParsed'].dt.second + Dates['ExpecTimeParsed'].dt.minute * 60 + Dates['ExpecTimeParsed'].dt.hour * 3600) - (Dates['SchedTimeParsed'].dt.second + Dates['SchedTimeParsed'].dt.minute * 60 + Dates['SchedTimeParsed'].dt.hour * 3600)\n","\n","Dates['Late_yes'] = [1 if seconds > 900 else 0 for seconds in Dates['SecondsLate']]\n","Dates['Early_yes'] = [1 if seconds < 0 else 0 for seconds in Dates['SecondsLate']]\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ga01gJG2_ZnZ"},"source":["# dropping all redudent fields\n","Dates.drop(['SchedTimeParsed','ExpecTimeParsed','Day','SecondsLate'],axis=1, inplace=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NALWHb6g4vbM"},"source":["# Reseting index once more and reducing all numbers remaining\n","# to smallest posible value to save memory space. This is\n","# necessary due to the amount of features that have been created\n","# during feature engineering\n","Dates.reset_index(drop=True, inplace=True)\n","Dates = Dates.astype('int8')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CSAXwlualdV1"},"source":["#**Merge Data**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":484},"id":"PP4tEpCulfxJ","executionInfo":{"elapsed":454,"status":"ok","timestamp":1604797599976,"user":{"displayName":"Taco Cat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjwDT7Fd6ACtx22TxBnnnogMSGkapVQVe2K8C2q=s64","userId":"04847116057603355161"},"user_tz":360},"outputId":"f241a8a4-9d10-4dd8-9944-0b435b326ea7"},"source":["Dates.info()\n","Dates.head()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 1693248 entries, 0 to 1693247\n","Columns: 367 entries, DirectionRef to Early_yes\n","dtypes: int8(367)\n","memory usage: 592.6 MB\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>DirectionRef</th>\n","      <th>DistanceFromStop</th>\n","      <th>149 ST</th>\n","      <th>227 ST 114 AV via LIBERTY</th>\n","      <th>25 AV CROPSEY AV</th>\n","      <th>31 ST 6 AV</th>\n","      <th>41 ST via BROADWAY/7 AV</th>\n","      <th>42 ST PIER CROSSTOWN</th>\n","      <th>44 ST 6 AV</th>\n","      <th>ABINGDON SQ CROSSTOWN</th>\n","      <th>...</th>\n","      <th>X</th>\n","      <th>Hour</th>\n","      <th>Monday</th>\n","      <th>Saturday</th>\n","      <th>Sunday</th>\n","      <th>Thursday</th>\n","      <th>Tuesday</th>\n","      <th>Wednesday</th>\n","      <th>Late_yes</th>\n","      <th>Early_yes</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>-100</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>15</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>-49</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>1</td>\n","      <td>18</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>87</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>8</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>9</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>2</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>...</td>\n","      <td>0</td>\n","      <td>7</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5 rows × 367 columns</p>\n","</div>"],"text/plain":["   DirectionRef  DistanceFromStop  149 ST  227 ST 114 AV via LIBERTY  \\\n","0             0              -100       0                          0   \n","1             1               -49       0                          0   \n","2             0                87       0                          0   \n","3             1                11       0                          0   \n","4             0                 2       0                          0   \n","\n","   25 AV CROPSEY AV  31 ST 6 AV  41 ST via BROADWAY/7 AV  \\\n","0                 0           0                        0   \n","1                 0           0                        0   \n","2                 0           0                        0   \n","3                 0           0                        0   \n","4                 0           0                        0   \n","\n","   42 ST PIER CROSSTOWN  44 ST 6 AV  ABINGDON SQ CROSSTOWN  ...  X  Hour  \\\n","0                     0           0                      0  ...  0    15   \n","1                     0           0                      0  ...  1    18   \n","2                     0           0                      0  ...  0     8   \n","3                     0           0                      0  ...  0     9   \n","4                     0           0                      0  ...  0     7   \n","\n","   Monday  Saturday  Sunday  Thursday  Tuesday  Wednesday  Late_yes  Early_yes  \n","0       0         0       1         0        0          0         0          0  \n","1       0         0       0         1        0          0         0          0  \n","2       0         0       0         1        0          0         0          0  \n","3       0         0       0         0        0          0         0          0  \n","4       0         0       0         0        1          0         0          0  \n","\n","[5 rows x 367 columns]"]},"metadata":{"tags":[]},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"U6Vhj0twPLoc"},"source":["# Data is exported to CSV \n","# which can now be loaded to other notebooks for model building\n","# in order to save memory and reprocessing time during\n","# experimentation\n","Dates.to_csv(r'\\Final Supervised Model',index=False)"],"execution_count":null,"outputs":[]}]}